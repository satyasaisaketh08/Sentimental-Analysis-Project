{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"3e88b6ae-d468-4620-a96e-b028d3539df1","_cell_guid":"6a271117-dcaf-4922-a32b-98814a7f7757","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-05-09T17:09:19.004093Z","iopub.execute_input":"2022-05-09T17:09:19.004673Z","iopub.status.idle":"2022-05-09T17:09:20.320743Z","shell.execute_reply.started":"2022-05-09T17:09:19.004581Z","shell.execute_reply":"2022-05-09T17:09:20.320015Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**English Stopwords**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nstop_words=set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:20.322304Z","iopub.execute_input":"2022-05-09T17:09:20.322555Z","iopub.status.idle":"2022-05-09T17:09:20.470642Z","shell.execute_reply.started":"2022-05-09T17:09:20.322520Z","shell.execute_reply":"2022-05-09T17:09:20.469821Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Load the Dataset**","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/twitter-sentimental-analysis/bdaproj.csv\",encoding='latin-1')\n\ndata.columns = ['sentiment','id','date','flag','user','tweet']\ndata.sentiment = data.sentiment.map({4:1,0:0})","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:20.472129Z","iopub.execute_input":"2022-05-09T17:09:20.472413Z","iopub.status.idle":"2022-05-09T17:09:26.795133Z","shell.execute_reply.started":"2022-05-09T17:09:20.472376Z","shell.execute_reply":"2022-05-09T17:09:26.794404Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:26.797058Z","iopub.execute_input":"2022-05-09T17:09:26.797347Z","iopub.status.idle":"2022-05-09T17:09:26.819087Z","shell.execute_reply.started":"2022-05-09T17:09:26.797297Z","shell.execute_reply":"2022-05-09T17:09:26.818390Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Remove duplicate tweets**","metadata":{}},{"cell_type":"code","source":"init_count = len(data)\ntweet_unique = data.tweet.drop_duplicates(keep=False)\nrecords_with_same_tweet = data[True ^ data.tweet.isin(tweet_unique)]\nrecords_with_same_tweet[['sentiment', 'user', 'tweet']]\ndata=data[True ^ data.tweet.isin(records_with_same_tweet.tweet)]\ncurr_count = len(data)\nprint(f\"{init_count - curr_count} duplicate rows dropped!\\nCurrent row count: {curr_count}\\n\\n\")\n\ndata.sentiment.map({1:\"Positive(1)\",0:\"Negative(0)\"}).value_counts().plot(kind='bar', color=['green', 'red'])\nplt.title(\"Data distribution\", fontdict={\"fontsize\": 20})\nplt.show()\n\ndata","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:26.821292Z","iopub.execute_input":"2022-05-09T17:09:26.821698Z","iopub.status.idle":"2022-05-09T17:09:28.646860Z","shell.execute_reply.started":"2022-05-09T17:09:26.821662Z","shell.execute_reply":"2022-05-09T17:09:28.646210Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Drop Unwanted Columns**","metadata":{}},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:28.648241Z","iopub.execute_input":"2022-05-09T17:09:28.648730Z","iopub.status.idle":"2022-05-09T17:09:28.659849Z","shell.execute_reply.started":"2022-05-09T17:09:28.648692Z","shell.execute_reply":"2022-05-09T17:09:28.659123Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:28.660967Z","iopub.execute_input":"2022-05-09T17:09:28.661546Z","iopub.status.idle":"2022-05-09T17:09:28.679937Z","shell.execute_reply.started":"2022-05-09T17:09:28.661511Z","shell.execute_reply":"2022-05-09T17:09:28.679143Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the Tweet**","metadata":{}},{"cell_type":"code","source":"#changing into lowercase\n\ndata[\"tweet\"]=data[\"tweet\"].str.lower()\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:28.681370Z","iopub.execute_input":"2022-05-09T17:09:28.681703Z","iopub.status.idle":"2022-05-09T17:09:29.579760Z","shell.execute_reply.started":"2022-05-09T17:09:28.681647Z","shell.execute_reply":"2022-05-09T17:09:29.579093Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(data.isnull())","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:29.581117Z","iopub.execute_input":"2022-05-09T17:09:29.581400Z","iopub.status.idle":"2022-05-09T17:09:41.109829Z","shell.execute_reply.started":"2022-05-09T17:09:29.581360Z","shell.execute_reply":"2022-05-09T17:09:41.109117Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Remove Noise**","metadata":{}},{"cell_type":"code","source":"# abbreviation check list\nabbreviations = {\n        \"$\" : \" dollar \",\n        \"€\" : \" euro \",\n        \"4ao\" : \"for adults only\",\n        \"a.m\" : \"before midday\",\n        \"a3\" : \"anytime anywhere anyplace\",\n        \"aamof\" : \"as a matter of fact\",\n        \"acct\" : \"account\",\n        \"adih\" : \"another day in hell\",\n        \"afaic\" : \"as far as i am concerned\",\n        \"afaict\" : \"as far as i can tell\",\n        \"afaik\" : \"as far as i know\",\n        \"afair\" : \"as far as i remember\",\n        \"afk\" : \"away from keyboard\",\n        \"app\" : \"application\",\n        \"approx\" : \"approximately\",\n        \"apps\" : \"applications\",\n        \"asap\" : \"as soon as possible\",\n        \"asl\" : \"age, sex, location\",\n        \"atk\" : \"at the keyboard\",\n        \"ave.\" : \"avenue\",\n        \"aymm\" : \"are you my mother\",\n        \"ayor\" : \"at your own risk\", \n        \"b&b\" : \"bed and breakfast\",\n        \"b+b\" : \"bed and breakfast\",\n        \"b.c\" : \"before christ\",\n        \"b2b\" : \"business to business\",\n        \"b2c\" : \"business to customer\",\n        \"b4\" : \"before\",\n        \"b4n\" : \"bye for now\",\n        \"b@u\" : \"back at you\",\n        \"bae\" : \"before anyone else\",\n        \"bak\" : \"back at keyboard\",\n        \"bbbg\" : \"bye bye be good\",\n        \"bbc\" : \"british broadcasting corporation\",\n        \"bbias\" : \"be back in a second\",\n        \"bbl\" : \"be back later\",\n        \"bbs\" : \"be back soon\",\n        \"be4\" : \"before\",\n        \"bfn\" : \"bye for now\",\n        \"blvd\" : \"boulevard\",\n        \"bout\" : \"about\",\n        \"brb\" : \"be right back\",\n        \"bros\" : \"brothers\",\n        \"brt\" : \"be right there\",\n        \"bsaaw\" : \"big smile and a wink\",\n        \"btw\" : \"by the way\",\n        \"bwl\" : \"bursting with laughter\",\n        \"c/o\" : \"care of\",\n        \"cet\" : \"central european time\",\n        \"cf\" : \"compare\",\n        \"cia\" : \"central intelligence agency\",\n        \"csl\" : \"can not stop laughing\",\n        \"cu\" : \"see you\",\n        \"cul8r\" : \"see you later\",\n        \"cv\" : \"curriculum vitae\",\n        \"cwot\" : \"complete waste of time\",\n        \"cya\" : \"see you\",\n        \"cyt\" : \"see you tomorrow\",\n        \"dae\" : \"does anyone else\",\n        \"dbmib\" : \"do not bother me i am busy\",\n        \"diy\" : \"do it yourself\",\n        \"dm\" : \"direct message\",\n        \"dwh\" : \"during work hours\",\n        \"e123\" : \"easy as one two three\",\n        \"eet\" : \"eastern european time\",\n        \"eg\" : \"example\",\n        \"embm\" : \"early morning business meeting\",\n        \"encl\" : \"enclosed\",\n        \"encl.\" : \"enclosed\",\n        \"etc\" : \"and so on\",\n        \"faq\" : \"frequently asked questions\",\n        \"fawc\" : \"for anyone who cares\",\n        \"fb\" : \"facebook\",\n        \"fc\" : \"fingers crossed\",\n        \"fig\" : \"figure\",\n        \"fimh\" : \"forever in my heart\", \n        \"ft.\" : \"feet\",\n        \"ft\" : \"featuring\",\n        \"ftl\" : \"for the loss\",\n        \"ftw\" : \"for the win\",\n        \"fwiw\" : \"for what it is worth\",\n        \"fyi\" : \"for your information\",\n        \"g9\" : \"genius\",\n        \"gahoy\" : \"get a hold of yourself\",\n        \"gal\" : \"get a life\",\n        \"gcse\" : \"general certificate of secondary education\",\n        \"gfn\" : \"gone for now\",\n        \"gg\" : \"good game\",\n        \"gl\" : \"good luck\",\n        \"glhf\" : \"good luck have fun\",\n        \"gmt\" : \"greenwich mean time\",\n        \"gmta\" : \"great minds think alike\",\n        \"gn\" : \"good night\",\n        \"g.o.a.t\" : \"greatest of all time\",\n        \"goat\" : \"greatest of all time\",\n        \"goi\" : \"get over it\",\n        \"gps\" : \"global positioning system\",\n        \"gr8\" : \"great\",\n        \"gratz\" : \"congratulations\",\n        \"gyal\" : \"girl\",\n        \"h&c\" : \"hot and cold\",\n        \"hp\" : \"horsepower\",\n        \"hr\" : \"hour\",\n        \"hrh\" : \"his royal highness\",\n        \"ht\" : \"height\",\n        \"ibrb\" : \"i will be right back\",\n        \"ic\" : \"i see\",\n        \"icq\" : \"i seek you\",\n        \"icymi\" : \"in case you missed it\",\n        \"idc\" : \"i do not care\",\n        \"idgadf\" : \"i do not give a damn fuck\",\n        \"idgaf\" : \"i do not give a fuck\",\n        \"idk\" : \"i do not know\",\n        \"ie\" : \"that is\",\n        \"i.e\" : \"that is\",\n        \"ifyp\" : \"i feel your pain\",\n        \"ig\" : \"instagram\",\n        \"iirc\" : \"if i remember correctly\",\n        \"ilu\" : \"i love you\",\n        \"ily\" : \"i love you\",\n        \"imho\" : \"in my humble opinion\",\n        \"imo\" : \"in my opinion\",\n        \"imu\" : \"i miss you\",\n        \"iow\" : \"in other words\",\n        \"irl\" : \"in real life\",\n        \"j4f\" : \"just for fun\",\n        \"jic\" : \"just in case\",\n        \"jk\" : \"just kidding\",\n        \"jsyk\" : \"just so you know\",\n        \"l8r\" : \"later\",\n        \"lb\" : \"pound\",\n        \"lbs\" : \"pounds\",\n        \"ldr\" : \"long distance relationship\",\n        \"lmao\" : \"laugh my ass off\",\n        \"lmfao\" : \"laugh my fucking ass off\",\n        \"lol\" : \"laughing out loud\",\n        \"ltd\" : \"limited\",\n        \"ltns\" : \"long time no see\",\n        \"m8\" : \"mate\",\n        \"mf\" : \"motherfucker\",\n        \"mfs\" : \"motherfuckers\",\n        \"mfw\" : \"my face when\",\n        \"mofo\" : \"motherfucker\",\n        \"mph\" : \"miles per hour\",\n        \"mr\" : \"mister\",\n        \"mrw\" : \"my reaction when\",\n        \"ms\" : \"miss\",\n        \"mte\" : \"my thoughts exactly\",\n        \"nagi\" : \"not a good idea\",\n        \"nbc\" : \"national broadcasting company\",\n        \"nbd\" : \"not big deal\",\n        \"nfs\" : \"not for sale\",\n        \"ngl\" : \"not going to lie\",\n        \"nhs\" : \"national health service\",\n        \"nrn\" : \"no reply necessary\",\n        \"nsfl\" : \"not safe for life\",\n        \"nsfw\" : \"not safe for work\",\n        \"nth\" : \"nice to have\",\n        \"nvr\" : \"never\",\n        \"nyc\" : \"new york city\",\n        \"oc\" : \"original content\",\n        \"og\" : \"original\",\n        \"ohp\" : \"overhead projector\",\n        \"oic\" : \"oh i see\",\n        \"omdb\" : \"over my dead body\",\n        \"omg\" : \"oh my god\",\n        \"omw\" : \"on my way\",\n        \"p.a\" : \"per annum\",\n        \"p.m\" : \"after midday\",\n        \"pm\" : \"prime minister\",\n        \"poc\" : \"people of color\",\n        \"pov\" : \"point of view\",\n        \"pp\" : \"pages\",\n        \"ppl\" : \"people\",\n        \"prw\" : \"parents are watching\",\n        \"ps\" : \"postscript\",\n        \"pt\" : \"point\",\n        \"ptb\" : \"please text back\",\n        \"pto\" : \"please turn over\",\n        \"qpsa\" : \"what happens\", \n        \"ratchet\" : \"rude\",\n        \"rbtl\" : \"read between the lines\",\n        \"rlrt\" : \"real life retweet\", \n        \"rofl\" : \"rolling on the floor laughing\",\n        \"roflol\" : \"rolling on the floor laughing out loud\",\n        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n        \"rt\" : \"retweet\",\n        \"ruok\" : \"are you ok\",\n        \"sfw\" : \"safe for work\",\n        \"sk8\" : \"skate\",\n        \"smh\" : \"shake my head\",\n        \"sq\" : \"square\",\n        \"srsly\" : \"seriously\", \n        \"ssdd\" : \"same stuff different day\",\n        \"tbh\" : \"to be honest\",\n        \"tbs\" : \"tablespooful\",\n        \"tbsp\" : \"tablespooful\",\n        \"tfw\" : \"that feeling when\",\n        \"thks\" : \"thank you\",\n        \"tho\" : \"though\",\n        \"thx\" : \"thank you\",\n        \"tia\" : \"thanks in advance\",\n        \"til\" : \"today i learned\",\n        \"tl;dr\" : \"too long i did not read\",\n        \"tldr\" : \"too long i did not read\",\n        \"tmb\" : \"tweet me back\",\n        \"tntl\" : \"trying not to laugh\",\n        \"ttyl\" : \"talk to you later\",\n        \"u\" : \"you\",\n        \"u2\" : \"you too\",\n        \"u4e\" : \"yours for ever\",\n        \"utc\" : \"coordinated universal time\",\n        \"w/\" : \"with\",\n        \"w/o\" : \"without\",\n        \"w8\" : \"wait\",\n        \"wassup\" : \"what is up\",\n        \"wb\" : \"welcome back\",\n        \"wtf\" : \"what the fuck\",\n        \"wtg\" : \"way to go\",\n        \"wtpa\" : \"where the party at\",\n        \"wuf\" : \"where are you from\",\n        \"wuzup\" : \"what is up\",\n        \"wywh\" : \"wish you were here\",\n        \"yd\" : \"yard\",\n        \"ygtr\" : \"you got that right\",\n        \"ynk\" : \"you never know\",\n        \"zzz\" : \"sleeping bored and tired\"\n    }\n\ndef decontract(text):\n    # remove special chars\n    text = re.sub(r\"\\x89Û_\", \"\", text)\n    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n    text = re.sub(r\"\\x89Û÷\", \"\", text)\n    text = re.sub(r\"\\x89Ûª\", \"\", text)\n    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n    text = re.sub(r\"å_\", \"\", text)\n    text = re.sub(r\"\\x89Û¢\", \"\", text)\n    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n    text = re.sub(r\"åÊ\", \"\", text)\n    text = re.sub(r\"åÈ\", \"\", text)\n    text = re.sub(r\"Ì©\", \"e\", text)\n    text = re.sub(r\"å¨\", \"\", text)\n    text = re.sub(r\"åÇ\", \"\", text)\n    text = re.sub(r\"åÀ\", \"\", text)\n    # remove contractions\n    text = re.sub(r\"let\\x89Ûªs\", \"let us\", text)\n    text = re.sub(r\"let's\", \"let us\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"we're\", \"we are\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"that\\x89Ûªs\", \"that is\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"wont\", \"will not\", text)\n    text = re.sub(r\"they're\", \"they are\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"cant\", \"cannot\", text)\n    text = re.sub(r\"can\\x89Ûªt\", \"cannot\", text)\n    text = re.sub(r\"wasn't\", \"was not\", text)\n    text = re.sub(r\"wasnt\", \"was not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"dont\", \"do not\", text)\n    text = re.sub(r\"donå«t\", \"do not\", text)  \n    text = re.sub(r\"don\\x89Ûªt\", \"do not\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"didnt\", \"did not\", text)\n    text = re.sub(r\"aren't\", \"are not\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"haven't\", \"have not\", text)\n    text = re.sub(r\"hasn't\", \"has not\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"it\\x89Ûªs\", \"it is\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"you\\x89Ûªre\", \"you are\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"i\\x89Ûªm\", \"i am\", text)\n    text = re.sub(r\"shoulda\", \"should have\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"wouldn't\", \"would not\", text)\n    text = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"here\\x89Ûªs\", \"here is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"you've\", \"you have\", text)\n    text = re.sub(r\"you\\x89Ûªve\", \"you have\", text)\n    text = re.sub(r\"youve\", \"you have\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"we've\", \"we have\", text)\n    text = re.sub(r\"doesn't\", \"does not\", text)\n    text = re.sub(r\"doesn\\x89Ûªt\", \"does not\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"i've\", \"i have\", text)\n    text = re.sub(r\"i\\x89Ûªve\", \"i have\", text)\n    text = re.sub(r\"y'all\", \"you all\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"it'll\", \"it will\", text)\n    text = re.sub(r\"we'll\", \"we will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"weren't\", \"were not\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"they'll\", \"they will\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)\n    text = re.sub(r\"they've\", \"they have\", text)\n    text = re.sub(r\"i'd\", \"i would\", text)\n    text = re.sub(r\"I\\x89Ûªd\", \"I would\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"we'd\", \"we would\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"^ill$\", \"i will\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"you\\x89Ûªll\", \"you will\", text)    \n    text = re.sub(r\"ain't\", \"am not\", text)    \n    text = re.sub(r\"you'd\", \"you would\", text)\n    text = re.sub(r\"could've\", \"could have\", text)\n    text = re.sub(r\"mÌ¼sica\", \"music\", text)\n    text = re.sub(r\"some1\", \"someone\", text)\n    text = re.sub(r\"yrs\", \"years\", text)\n    text = re.sub(r\"hrs\", \"hours\", text)\n    text = re.sub(r\"2morow|2moro\", \"tomorrow\", text)\n    text = re.sub(r\"2day\", \"today\", text)\n    text = re.sub(r\"4got|4gotten\", \"forget\", text)\n    text = re.sub(r\"b-day|bday\", \"b-day\", text)\n    text = re.sub(r\"mother's\", \"mother\", text)\n    text = re.sub(r\"mom's\", \"mom\", text)\n    text = re.sub(r\"dad's\", \"dad\", text)\n    text = re.sub(r\"^[h|a]+$\", \"haha\", text)\n    text = re.sub(r\"lmao|lolz|rofl\", \"lol\", text)\n    text = re.sub(r\"thanx|thnx|thx\", \"thanks\", text)\n    text = re.sub(r'all[l]+', \"all\", text)\n    text = re.sub(r'so[o]+', \"so\", text)\n    text = re.sub(r'a[w]+', \"awww\", text)\n    text = re.sub(r'why[y]+', \"why\", text)\n    text = re.sub(r'way[y]+', \"way\", text)\n    text = re.sub(r'will[l]+', \"will\", text)\n    text = re.sub(r'oo[o]+h', \"ooh\", text)\n    text = re.sub(r'hey[y]+', \"hey\", text)\n    text = re.sub(r\"boo[o]+m\", \"boom\", text)\n    text = re.sub(r\"co[o]+ld\", \"cold\", text)\n    text = re.sub(r\"goo[o]+d\", \"good\", text)\n    text = re.sub(r\"luckigrrl\", \"lucky girl\", text)\n    text = re.sub(r\"evolvin\", \"evolving\", text)\n\n    # specific\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"@\", \"\" , text)         # removal of @\n    text = re.sub(r\"http\\S+\", \"\", text)   # removal of URLs\n    text = re.sub(r\"#\", \"\", text)          # hashtag processing\n\n    # general\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n\n    # deal with some abbreviations\n    words = text.split()\n    text = ' '.join([abbreviations[word] if word in abbreviations.keys() else word.strip(string.punctuation) for word in words])\n\n    # character entity references\n    text = re.sub(r\"&gt;\", \">\", text)\n    text = re.sub(r\"&lt;\", \"<\", text)\n    text = re.sub(r\"&amp;\", \"&\", text)\n\n    # typos, slang and informal abbreviations\n    text = re.sub(r\"w/e\", \"whatever\", text)\n    text = re.sub(r\"usagov\", \"usa government\", text)\n    text = re.sub(r\"<3\", \"love\", text)\n    text = re.sub(r\"trfc\", \"traffic\", text)\n    \n    # remove mentions\n    text = re.sub(r'^@[0-9a-zA-Z_]+', \"\", text)\n\n    # words with punctuations and special characters\n    for punc in string.punctuation:\n        text = text.replace(punc, '')\n\n    # ... and ..\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n\n    return text\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words])\n\ndef lemmatize_data(text):\n    lemmatizer=WordNetLemmatizer()\n    lemma_words=[lemmatizer.lemmatize(word) for word in str(text).split()]\n    return \" \".join(lemma_words)\n\ndef remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)\n\ndef remove_email(text):\n    return re.sub('@[^\\s]+', ' ', text)\n\ndef remove_URLs(text):\n    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n\ndef remove_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndef clean_tweets(text):\n    text = decontract(text)\n    text = remove_stopwords(text)\n    text = remove_email(text)\n    text = remove_URLs(text)\n    return text\n\ninit_rowcount = len(data)\ndata['tokens'] = data.tweet.apply(lambda tweet: clean_tweets(tweet))\ndata['length'] = data.tokens.str.len()\ndata = data[data.length != 0]\ncurr_rowcount = len(data)\navg_tweet_length = data.length.mode().iat[0]\nmin_tweet_length = data.length.min()\nprint(f\"functions applied!\\n\\n{init_rowcount - curr_rowcount} rows with zero-length tweets dropped!\\nMin tweet length observed: {min_tweet_length} words\\nAvg tweet length observed: {avg_tweet_length} words\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:09:41.113347Z","iopub.execute_input":"2022-05-09T17:09:41.113552Z","iopub.status.idle":"2022-05-09T17:14:37.060441Z","shell.execute_reply.started":"2022-05-09T17:09:41.113525Z","shell.execute_reply":"2022-05-09T17:14:37.059694Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:14:37.061609Z","iopub.execute_input":"2022-05-09T17:14:37.062283Z","iopub.status.idle":"2022-05-09T17:14:37.075084Z","shell.execute_reply.started":"2022-05-09T17:14:37.062231Z","shell.execute_reply":"2022-05-09T17:14:37.074312Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# data = data[['sentiment', 'tokens']]\n# data.rename(columns={'tokens': 'tweet'}, inplace=True)\n# data.to_csv()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:14:37.076491Z","iopub.execute_input":"2022-05-09T17:14:37.076963Z","iopub.status.idle":"2022-05-09T17:14:37.080492Z","shell.execute_reply.started":"2022-05-09T17:14:37.076924Z","shell.execute_reply":"2022-05-09T17:14:37.079769Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = data[['sentiment', 'tokens']]\ndata.rename(columns={'tokens': 'tweet'}, inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:14:37.081782Z","iopub.execute_input":"2022-05-09T17:14:37.082228Z","iopub.status.idle":"2022-05-09T17:14:37.123623Z","shell.execute_reply.started":"2022-05-09T17:14:37.082190Z","shell.execute_reply":"2022-05-09T17:14:37.122942Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Wordcloud by class (positive/Negative)**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:14:37.124953Z","iopub.execute_input":"2022-05-09T17:14:37.125419Z","iopub.status.idle":"2022-05-09T17:14:37.154409Z","shell.execute_reply.started":"2022-05-09T17:14:37.125382Z","shell.execute_reply":"2022-05-09T17:14:37.153777Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 1].tweet))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.title('Tweets positifs')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:14:37.155701Z","iopub.execute_input":"2022-05-09T17:14:37.156137Z","iopub.status.idle":"2022-05-09T17:15:44.274156Z","shell.execute_reply.started":"2022-05-09T17:14:37.156101Z","shell.execute_reply":"2022-05-09T17:15:44.273531Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,15)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 0].tweet))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.title('Tweets négatifs')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:15:44.275073Z","iopub.execute_input":"2022-05-09T17:15:44.275298Z","iopub.status.idle":"2022-05-09T17:16:50.388449Z","shell.execute_reply.started":"2022-05-09T17:15:44.275268Z","shell.execute_reply":"2022-05-09T17:16:50.387740Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Splitting dataset in train and test**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(data['tweet'].values, data['sentiment'].values, test_size=0.25, random_state=1)\nprint(\"x_train : \", x_train.shape)\nprint(\"y_train : \", y_train.shape)\nprint(\"x_test : \", x_test.shape)\nprint(\"y_test : \", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:16:50.389474Z","iopub.execute_input":"2022-05-09T17:16:50.389821Z","iopub.status.idle":"2022-05-09T17:16:50.756442Z","shell.execute_reply.started":"2022-05-09T17:16:50.389781Z","shell.execute_reply":"2022-05-09T17:16:50.755720Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Vectorization with CountVectorizer and classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer=CountVectorizer()\n\ncountVec_train=count_vectorizer.fit_transform(x_train) \ncountVec_test=count_vectorizer.transform(x_test)\nprint(\"count_train : \", countVec_train.shape)\nprint(\"count_test : \", countVec_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:16:50.757855Z","iopub.execute_input":"2022-05-09T17:16:50.758118Z","iopub.status.idle":"2022-05-09T17:17:11.088347Z","shell.execute_reply.started":"2022-05-09T17:16:50.758082Z","shell.execute_reply":"2022-05-09T17:17:11.087530Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn import metrics\n# import seaborn as sns\n\n\n# rf_clf_countVec = RandomForestClassifier(n_estimators = 100)\n# rf_clf_countVec = rf_clf_countVec.fit(countVec_train, y_train)\n# y_pred = rf_clf_countVec.predict(countVec_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)\n\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn import metrics\n# import seaborn as sns\n\n# np.random.seed(0)\n# svm_clf_countVec = #svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n# svm_clf_countVec = svm_clf_countVec.fit(countVec_train, y_train)\n# y_pred = svm_clf_countVec.predict(countVec_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:17:11.089450Z","iopub.execute_input":"2022-05-09T17:17:11.089690Z","iopub.status.idle":"2022-05-09T17:17:11.097050Z","shell.execute_reply.started":"2022-05-09T17:17:11.089655Z","shell.execute_reply":"2022-05-09T17:17:11.096206Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**MultinomialNB**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport seaborn as sns\n\n\nnb_clf_countVec =MultinomialNB()\nnb_clf_countVec = nb_clf_countVec.fit(countVec_train, y_train)\ny_pred_nb_clf_countVec = nb_clf_countVec.predict(countVec_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_nb_clf_countVec))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_nb_clf_countVec))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_nb_clf_countVec))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_nb_clf_countVec),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:17:11.098357Z","iopub.execute_input":"2022-05-09T17:17:11.098850Z","iopub.status.idle":"2022-05-09T17:17:12.027349Z","shell.execute_reply.started":"2022-05-09T17:17:11.098808Z","shell.execute_reply":"2022-05-09T17:17:12.026623Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nfrom sklearn import metrics\n\nlr_clf_countVec = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_countVec = lr_clf_countVec.fit(countVec_train, y_train)\ny_pred_lr_clf_countVec = lr_clf_countVec.predict(countVec_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_lr_clf_countVec))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_lr_clf_countVec))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_lr_clf_countVec))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_lr_clf_countVec),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:17:12.028459Z","iopub.execute_input":"2022-05-09T17:17:12.029400Z","iopub.status.idle":"2022-05-09T17:19:06.263838Z","shell.execute_reply.started":"2022-05-09T17:17:12.029359Z","shell.execute_reply":"2022-05-09T17:19:06.263090Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Calculating False Positive Rate(fpr), True Positive Rate(tpr) , threshold for different models using roc curve**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred_nb_clf_countVec, pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, y_pred_lr_clf_countVec, pos_label=1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:06.265198Z","iopub.execute_input":"2022-05-09T17:19:06.265468Z","iopub.status.idle":"2022-05-09T17:19:06.420116Z","shell.execute_reply.started":"2022-05-09T17:19:06.265433Z","shell.execute_reply":"2022-05-09T17:19:06.419386Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Calculating ROC scores to all models**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nRoc=[]\nRoc.append(roc_auc_score(y_test,y_pred_nb_clf_countVec ))\nRoc.append(roc_auc_score(y_test, y_pred_lr_clf_countVec))\n\nprint(*Roc)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:06.421653Z","iopub.execute_input":"2022-05-09T17:19:06.421916Z","iopub.status.idle":"2022-05-09T17:19:06.590158Z","shell.execute_reply.started":"2022-05-09T17:19:06.421882Z","shell.execute_reply":"2022-05-09T17:19:06.589406Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Plotting all the ROC curves**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='MultinomialNB')\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Logistic Regression')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\nplt.title('ROC curve')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:06.591339Z","iopub.execute_input":"2022-05-09T17:19:06.591850Z","iopub.status.idle":"2022-05-09T17:19:07.139449Z","shell.execute_reply.started":"2022-05-09T17:19:06.591804Z","shell.execute_reply":"2022-05-09T17:19:07.138781Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Predicting best model based on roc score**","metadata":{}},{"cell_type":"code","source":"if Roc.index(max(Roc))==0:\n    print(\"MultinomialNB performs best\")\nelse:\n    print(\"LogisticRegression performs best\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:07.140515Z","iopub.execute_input":"2022-05-09T17:19:07.141416Z","iopub.status.idle":"2022-05-09T17:19:07.146639Z","shell.execute_reply.started":"2022-05-09T17:19:07.141376Z","shell.execute_reply":"2022-05-09T17:19:07.145677Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Vectorization with Tfidf (Term Frequency - Inverse Document) and classification**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer=TfidfVectorizer()\n\ntfidf_train=tfidf_vectorizer.fit_transform(x_train) \ntfidf_test=tfidf_vectorizer.transform(x_test)\nprint(\"tfidf_train : \", tfidf_train.shape)                                                                                                                                                                                                                                                    \nprint(\"tfidf_test : \", tfidf_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:07.148040Z","iopub.execute_input":"2022-05-09T17:19:07.148336Z","iopub.status.idle":"2022-05-09T17:19:27.958263Z","shell.execute_reply.started":"2022-05-09T17:19:07.148289Z","shell.execute_reply":"2022-05-09T17:19:27.956695Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn import metrics\n# import seaborn as sns\n\n# rf_clf_tfidf = RandomForestClassifier(n_estimators = 100)\n# rf_clf_tfidf = rf_clf_tfidf.fit(tfidf_train, y_train)\n# y_pred = rf_clf_tfidf.predict(tfidf_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)\n\n# from sklearn import svm\n# from sklearn import metrics\n# import seaborn as sns\n\n# np.random.seed(0)\n# svm_clf_tfidf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n# svm_clf_tfidf = svm_clf_tfidf.fit(tfidf_train, y_train)\n# y_pred = svm_clf_tfidf.predict(tfidf_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:27.959565Z","iopub.execute_input":"2022-05-09T17:19:27.959884Z","iopub.status.idle":"2022-05-09T17:19:27.965104Z","shell.execute_reply.started":"2022-05-09T17:19:27.959847Z","shell.execute_reply":"2022-05-09T17:19:27.964233Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Multinomial NB**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport seaborn as sns\n\nnb_clf_tfidf = MultinomialNB()\nnb_clf_tfidf = nb_clf_tfidf.fit(tfidf_train, y_train)\ny_pred_nb_clf_tfidf = nb_clf_tfidf.predict(tfidf_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_nb_clf_tfidf))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_nb_clf_tfidf))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_nb_clf_tfidf))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_nb_clf_tfidf),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:27.972562Z","iopub.execute_input":"2022-05-09T17:19:27.973371Z","iopub.status.idle":"2022-05-09T17:19:28.919813Z","shell.execute_reply.started":"2022-05-09T17:19:27.973332Z","shell.execute_reply":"2022-05-09T17:19:28.919101Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport seaborn as sns\n\nlr_clf_tfidf = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_tfidf = lr_clf_tfidf.fit(tfidf_train, y_train)\ny_pred_lr_clf_tfidf = lr_clf_tfidf.predict(tfidf_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_lr_clf_tfidf))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_lr_clf_tfidf))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_lr_clf_tfidf))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_lr_clf_tfidf),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:28.921078Z","iopub.execute_input":"2022-05-09T17:19:28.921337Z","iopub.status.idle":"2022-05-09T17:19:57.219794Z","shell.execute_reply.started":"2022-05-09T17:19:28.921293Z","shell.execute_reply":"2022-05-09T17:19:57.219130Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"**Calculating False Positive Rate(fpr), True Positive Rate(tpr) , threshold for different models using roc curve**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred_nb_clf_tfidf, pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, y_pred_lr_clf_tfidf, pos_label=1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:57.221075Z","iopub.execute_input":"2022-05-09T17:19:57.221345Z","iopub.status.idle":"2022-05-09T17:19:57.363128Z","shell.execute_reply.started":"2022-05-09T17:19:57.221310Z","shell.execute_reply":"2022-05-09T17:19:57.362411Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"**Calculating ROC scores to all models**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nRoc=[]\nRoc.append(roc_auc_score(y_test,y_pred_nb_clf_tfidf ))\nRoc.append(roc_auc_score(y_test, y_pred_lr_clf_tfidf))\n\nprint(*Roc)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:57.364541Z","iopub.execute_input":"2022-05-09T17:19:57.364803Z","iopub.status.idle":"2022-05-09T17:19:57.534301Z","shell.execute_reply.started":"2022-05-09T17:19:57.364769Z","shell.execute_reply":"2022-05-09T17:19:57.532710Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"**Plotting all the ROC curves**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='MultinomialNB')\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Logistic Regression')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\nplt.title('ROC curve')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:57.535626Z","iopub.execute_input":"2022-05-09T17:19:57.535971Z","iopub.status.idle":"2022-05-09T17:19:58.082778Z","shell.execute_reply.started":"2022-05-09T17:19:57.535929Z","shell.execute_reply":"2022-05-09T17:19:58.082107Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**Predicting best model based on roc score**","metadata":{}},{"cell_type":"code","source":"if(Roc.index(max(Roc)))==0:\n    print(\"MultinomialNB performs best\")\nelse:\n    print(\"LogisticRegression performs best\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:58.083912Z","iopub.execute_input":"2022-05-09T17:19:58.085420Z","iopub.status.idle":"2022-05-09T17:19:58.090831Z","shell.execute_reply.started":"2022-05-09T17:19:58.085378Z","shell.execute_reply":"2022-05-09T17:19:58.090028Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"**Vectorization with Word2Vec trained with Gensim on this corpus**","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade gensim\nimport gensim\nprint(gensim.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:19:58.092809Z","iopub.execute_input":"2022-05-09T17:19:58.093011Z","iopub.status.idle":"2022-05-09T17:20:12.012641Z","shell.execute_reply.started":"2022-05-09T17:19:58.092987Z","shell.execute_reply":"2022-05-09T17:20:12.011872Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nclass MyCorpus:\n    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n\n    def __iter__(self):\n        \n        for sentence in data['tweet'].values:\n            yield sentence.split()\n\nsentences = MyCorpus()\n# I eliminate tokens whose frequency is less than 10\ntrained_w2v = Word2Vec(sentences=sentences, min_count=10, vector_size=100)\nprint(trained_w2v)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:20:12.013983Z","iopub.execute_input":"2022-05-09T17:20:12.015445Z","iopub.status.idle":"2022-05-09T17:22:05.094714Z","shell.execute_reply.started":"2022-05-09T17:20:12.015412Z","shell.execute_reply":"2022-05-09T17:22:05.093939Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import IncrementalPCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nnp.random.seed(0)\n\ndef visualize_embeddings(wv):\n    num_dimensions = 2\n\n    vectors = np.asarray(wv.vectors)\n    labels = np.asarray(wv.index_to_key)\n\n    tsne = TSNE(n_components=num_dimensions, random_state=0)\n    vectors = tsne.fit_transform(vectors)\n\n    x_vals = [v[0] for v in vectors]\n    y_vals = [v[1] for v in vectors]\n    \n    random.seed(0)\n\n    plt.figure(figsize=(12, 12))\n    plt.scatter(x_vals, y_vals)\n\n    indices = list(range(len(labels)))\n    selected_indices = random.sample(indices, 25)\n    for i in selected_indices:\n        plt.annotate(labels[i], (x_vals[i], y_vals[i]))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:22:05.096075Z","iopub.execute_input":"2022-05-09T17:22:05.096337Z","iopub.status.idle":"2022-05-09T17:22:05.184223Z","shell.execute_reply.started":"2022-05-09T17:22:05.096300Z","shell.execute_reply":"2022-05-09T17:22:05.183492Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"visualize_embeddings(trained_w2v.wv)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:22:05.185427Z","iopub.execute_input":"2022-05-09T17:22:05.185814Z","iopub.status.idle":"2022-05-09T17:32:30.163729Z","shell.execute_reply.started":"2022-05-09T17:22:05.185777Z","shell.execute_reply":"2022-05-09T17:32:30.163118Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Sentences embeddings**","metadata":{}},{"cell_type":"code","source":"def w2v_of_sentences(sentences, wv):\n    vectors = np.zeros((len(sentences), wv.vector_size))\n    for i, sentence in enumerate(sentences):\n        sum_vector = np.zeros((1, wv.vector_size))\n        tokens = sentence.split()\n        for token in tokens:\n            try:\n              sum_vector += wv[token]\n            except:\n              pass\n        if len(tokens) > 0:\n          vectors[i] = sum_vector / len(tokens)\n        else:\n          vectors[i] = sum_vector\n    return vectors","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:32:30.165118Z","iopub.execute_input":"2022-05-09T17:32:30.165554Z","iopub.status.idle":"2022-05-09T17:32:30.172456Z","shell.execute_reply.started":"2022-05-09T17:32:30.165519Z","shell.execute_reply":"2022-05-09T17:32:30.171770Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"w2v_train=w2v_of_sentences(x_train, trained_w2v.wv)\nw2v_test=w2v_of_sentences(x_test, trained_w2v.wv)\nprint(\"w2v_train : \", w2v_train.shape)\nprint(\"w2v_test : \", w2v_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:32:30.173684Z","iopub.execute_input":"2022-05-09T17:32:30.174001Z","iopub.status.idle":"2022-05-09T17:33:33.530411Z","shell.execute_reply.started":"2022-05-09T17:32:30.173964Z","shell.execute_reply":"2022-05-09T17:33:33.529606Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport seaborn as sns\n\nlr_clf_trained_w2v = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_trained_w2v = lr_clf_trained_w2v.fit(w2v_train, y_train)\ny_pred_lr_clf_trained_w2v = lr_clf_trained_w2v.predict(w2v_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_lr_clf_trained_w2v))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_lr_clf_trained_w2v))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_lr_clf_trained_w2v))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_lr_clf_trained_w2v),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:33:33.531676Z","iopub.execute_input":"2022-05-09T17:33:33.531943Z","iopub.status.idle":"2022-05-09T17:34:12.716877Z","shell.execute_reply.started":"2022-05-09T17:33:33.531906Z","shell.execute_reply":"2022-05-09T17:34:12.716129Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"**Support Vector Machine**","metadata":{}},{"cell_type":"code","source":"# from sklearn.naive_bayes import RandomForestClassifier\n# from sklearn import metrics\n# import seaborn as sns\n\n# rf_clf_trained_w2v = RandomForestClassifier(n_estimators = 100)\n# rf_clf_trained_w2v = rf_clf_trained_w2v.fit(w2v_train, y_train)\n# y_pred = rf_clf_trained_w2v.predict(w2v_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)\n\n# from sklearn import svm\n# from sklearn import metrics\n# import seaborn as sns\n\n# np.random.seed(0)\n# svm_clf_trained_w2v = svm.SVC(C=1.0, kernel='linear')\n# svm_clf_trained_w2v = svm_clf_trained_w2v.fit(w2v_train, y_train)\n# y_pred_svm_clf_trained_w2v = svm_clf_trained_w2v.predict(w2v_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_svm_clf_trained_w2v))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred_svm_clf_trained_w2v))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred_svm_clf_trained_w2v))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred_svm_clf_trained_w2v),annot=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:12.718076Z","iopub.execute_input":"2022-05-09T17:34:12.718367Z","iopub.status.idle":"2022-05-09T17:34:12.722403Z","shell.execute_reply.started":"2022-05-09T17:34:12.718319Z","shell.execute_reply":"2022-05-09T17:34:12.721676Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"**Calculating False Positive Rate(fpr), True Positive Rate(tpr) , threshold for different models using roc curve**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred_lr_clf_trained_w2v, pos_label=1)\n# fpr2, tpr2, thresh2 = roc_curve(y_test, y_pred_svm_clf_trained_w2v, pos_label=1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:12.723823Z","iopub.execute_input":"2022-05-09T17:34:12.724318Z","iopub.status.idle":"2022-05-09T17:34:12.846902Z","shell.execute_reply.started":"2022-05-09T17:34:12.724278Z","shell.execute_reply":"2022-05-09T17:34:12.846153Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"**Calculating ROC scores to all models**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nRoc=[]\nRoc.append(roc_auc_score(y_test,y_pred_lr_clf_trained_w2v ))\n#Roc.append(roc_auc_score(y_test, y_pred_svm_clf_trained_w2v))\n\nprint(*Roc)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:12.848425Z","iopub.execute_input":"2022-05-09T17:34:12.848713Z","iopub.status.idle":"2022-05-09T17:34:12.938080Z","shell.execute_reply.started":"2022-05-09T17:34:12.848675Z","shell.execute_reply":"2022-05-09T17:34:12.937245Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**Plotting all the ROC curves**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\n# plt.plot(fpr2, tpr2, linestyle='--',color='green', label='Support Vector Machine')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\nplt.title('ROC curve')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:12.939416Z","iopub.execute_input":"2022-05-09T17:34:12.940327Z","iopub.status.idle":"2022-05-09T17:34:13.621009Z","shell.execute_reply.started":"2022-05-09T17:34:12.940285Z","shell.execute_reply":"2022-05-09T17:34:13.620316Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"**Predicting best model based on roc score**","metadata":{}},{"cell_type":"code","source":"if Roc.index(max(Roc))==0:\n    print(\"LogisticRegression performs best\")\nelse:\n    print(\"SVM performs best\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:13.625101Z","iopub.execute_input":"2022-05-09T17:34:13.627091Z","iopub.status.idle":"2022-05-09T17:34:13.635939Z","shell.execute_reply.started":"2022-05-09T17:34:13.627052Z","shell.execute_reply":"2022-05-09T17:34:13.635117Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"**Vectorization with pre-trained word embeddings (GloVe)**","metadata":{}},{"cell_type":"markdown","source":"There are models already trained on tweets which are available through this link https://nlp.stanford.edu/data/glove.twitter.27B.zip. These models have been trained on a vocabulary of 1.2 million tokens.\n\nAfter having decompressed the archive, I will test the model whose vectors have a size of 50.\n\nI load the model with Gensim's load_word2vec_format function.","metadata":{}},{"cell_type":"markdown","source":"**Loading word embeddings with Gensim**","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n!unzip glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:34:13.637022Z","iopub.execute_input":"2022-05-09T17:34:13.637480Z","iopub.status.idle":"2022-05-09T17:39:40.564556Z","shell.execute_reply.started":"2022-05-09T17:34:13.637445Z","shell.execute_reply":"2022-05-09T17:39:40.563715Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"pretrained_glove_w2v = gensim.models.KeyedVectors.load_word2vec_format('glove.twitter.27B.50d.txt', binary=False, no_header=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:39:40.566606Z","iopub.execute_input":"2022-05-09T17:39:40.566902Z","iopub.status.idle":"2022-05-09T17:40:35.034093Z","shell.execute_reply.started":"2022-05-09T17:39:40.566861Z","shell.execute_reply":"2022-05-09T17:40:35.033307Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pretrained_glove_w2v_train=w2v_of_sentences(x_train, pretrained_glove_w2v)\npretrained_glove_w2v_test=w2v_of_sentences(x_test, pretrained_glove_w2v)\nprint(\"w2v_train : \", pretrained_glove_w2v_train.shape)\nprint(\"w2v_test : \", pretrained_glove_w2v_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:40:35.035577Z","iopub.execute_input":"2022-05-09T17:40:35.035847Z","iopub.status.idle":"2022-05-09T17:41:33.127549Z","shell.execute_reply.started":"2022-05-09T17:40:35.035810Z","shell.execute_reply":"2022-05-09T17:41:33.126017Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport seaborn as sns\n\nlr_clf_pretrained_glove_w2v = LogisticRegression(max_iter=100, solver='liblinear')\nlr_clf_pretrained_glove_w2v = lr_clf_pretrained_glove_w2v.fit(pretrained_glove_w2v_train, y_train)\ny_pred_lr_clf_pretrained_glove_w2v = lr_clf_pretrained_glove_w2v.predict(pretrained_glove_w2v_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_lr_clf_pretrained_glove_w2v))\nprint(\"precision:\",metrics.precision_score(y_test,y_pred_lr_clf_pretrained_glove_w2v))\nprint(\"recall:\",metrics.recall_score(y_test,y_pred_lr_clf_pretrained_glove_w2v))\nprint(\"Confusion Matrix\")\nsns.heatmap(metrics.confusion_matrix(y_test,y_pred_lr_clf_pretrained_glove_w2v),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:33.129077Z","iopub.execute_input":"2022-05-09T17:41:33.129415Z","iopub.status.idle":"2022-05-09T17:41:54.247814Z","shell.execute_reply.started":"2022-05-09T17:41:33.129370Z","shell.execute_reply":"2022-05-09T17:41:54.247099Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Support Vector Machine**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# import seaborn as sns\n\n# rf_clf_pretrained_glove_w2v = RandomForestClassifier(n_estimators = 100)\n# rf_clf_pretrained_glove_w2v = rf_clf_pretrained_glove_w2v.fit(pretrained_glove_w2v_train, y_train)\n# y_pred = rf_clf_pretrained_glove_w2v.predict(pretrained_glove_w2v_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred),annot=True)\n\n\n# from sklearn import svm\n# from sklearn import metrics\n# import seaborn as sns\n\n# np.random.seed(0)\n# svm_clf_pretrained_glove_w2v = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n# svm_clf_pretrained_glove_w2v = svm_clf_pretrained_glove_w2v.fit(pretrained_glove_w2v_train, y_train)\n# y_pred_svm_clf_pretrained_glove_w2v= svm_clf_pretrained_glove_w2v.predict(pretrained_glove_w2v_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_svm_clf_pretrained_glove_w2v))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred_svm_clf_pretrained_glove_w2v))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred_svm_clf_pretrained_glove_w2v))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred_svm_clf_pretrained_glove_w2v),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.248923Z","iopub.execute_input":"2022-05-09T17:41:54.249160Z","iopub.status.idle":"2022-05-09T17:41:54.254774Z","shell.execute_reply.started":"2022-05-09T17:41:54.249126Z","shell.execute_reply":"2022-05-09T17:41:54.254138Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"**Calculating False Positive Rate(fpr), True Positive Rate(tpr) , threshold for different models using roc curve**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred_lr_clf_pretrained_glove_w2v, pos_label=1)\n#fpr2, tpr2, thresh2 = roc_curve(y_test, y_pred_svm_clf_pretrained_glove_w2v, pos_label=1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.256099Z","iopub.execute_input":"2022-05-09T17:41:54.256366Z","iopub.status.idle":"2022-05-09T17:41:54.374692Z","shell.execute_reply.started":"2022-05-09T17:41:54.256330Z","shell.execute_reply":"2022-05-09T17:41:54.373974Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"**Calculating ROC scores to all models**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nRoc=[]\nRoc.append(roc_auc_score(y_test,y_pred_lr_clf_pretrained_glove_w2v ))\n#Roc.append(roc_auc_score(y_test, y_pred_svm_clf_pretrained_glove_w2v))\n\nprint(*Roc)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.376029Z","iopub.execute_input":"2022-05-09T17:41:54.376283Z","iopub.status.idle":"2022-05-09T17:41:54.462621Z","shell.execute_reply.started":"2022-05-09T17:41:54.376234Z","shell.execute_reply":"2022-05-09T17:41:54.461840Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Plotting all the ROC curves**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\n#plt.plot(fpr2, tpr2, linestyle='--',color='green', label='Support Vector Machine')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n\nplt.title('ROC curve')\n\nplt.xlabel('False Positive Rate')\n\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.463869Z","iopub.execute_input":"2022-05-09T17:41:54.465415Z","iopub.status.idle":"2022-05-09T17:41:54.981824Z","shell.execute_reply.started":"2022-05-09T17:41:54.465375Z","shell.execute_reply":"2022-05-09T17:41:54.981149Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"**Predicting best model based on roc score**","metadata":{}},{"cell_type":"code","source":"if Roc.index(max(Roc))==0:\n    print(\"LogisticRegression performs best\")\nelse:\n    print(\"SVM performs best\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.983105Z","iopub.execute_input":"2022-05-09T17:41:54.983676Z","iopub.status.idle":"2022-05-09T17:41:54.988988Z","shell.execute_reply.started":"2022-05-09T17:41:54.983634Z","shell.execute_reply":"2022-05-09T17:41:54.988275Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"print(\"success\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:54.990136Z","iopub.execute_input":"2022-05-09T17:41:54.990886Z","iopub.status.idle":"2022-05-09T17:41:54.999211Z","shell.execute_reply.started":"2022-05-09T17:41:54.990766Z","shell.execute_reply":"2022-05-09T17:41:54.998281Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"**Bar-Graph**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn import metrics\nx=[\"CountVec\",\"Tfidf\",\"Word2Vec\",\"Glove\"]\nh=[metrics.accuracy_score(y_test,y_pred_lr_clf_countVec),metrics.accuracy_score(y_test,y_pred_nb_clf_tfidf),metrics.accuracy_score(y_test,y_pred_lr_clf_trained_w2v),metrics.accuracy_score(y_test,y_pred_lr_clf_pretrained_glove_w2v)]\nc=[\"red\",\"green\",\"blue\",\"yellow\"]\nplt.bar(x,h,width=0.5,color=c)\nplt.xlabel(\"Different Vectorization Tecniques\")\nplt.ylabel(\"Accuracy attained by Logistic Regression\")\nplt.title(\"Finding best tecnique with best accuracy\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:55.000697Z","iopub.execute_input":"2022-05-09T17:41:55.001198Z","iopub.status.idle":"2022-05-09T17:41:55.313414Z","shell.execute_reply.started":"2022-05-09T17:41:55.001161Z","shell.execute_reply":"2022-05-09T17:41:55.312703Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"print(x[h.index(max(h))]+\"is predicted as the best technique, provided logistic regression As the common predictive model among all techniques\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:55.314503Z","iopub.execute_input":"2022-05-09T17:41:55.315317Z","iopub.status.idle":"2022-05-09T17:41:55.320202Z","shell.execute_reply.started":"2022-05-09T17:41:55.315278Z","shell.execute_reply":"2022-05-09T17:41:55.319493Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"!pip install joblib\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:41:55.321750Z","iopub.execute_input":"2022-05-09T17:41:55.322359Z","iopub.status.idle":"2022-05-09T17:42:05.045730Z","shell.execute_reply.started":"2022-05-09T17:41:55.322177Z","shell.execute_reply":"2022-05-09T17:42:05.044804Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model=LogisticRegression(solver=\"liblinear\")\nmodel.fit(countVec_train, y_train)\n\n# from sklearn.linear_model import LogisticRegression\n# import seaborn as sns\n# from sklearn import metrics\n\n# lr_clf_countVec = LogisticRegression(max_iter=100, solver='liblinear')\n# lr_clf_countVec = lr_clf_countVec.fit(countVec_train, y_train)\n# y_pred_lr_clf_countVec = lr_clf_countVec.predict(countVec_test)\n\n# print(\"Accuracy:\",metrics.accuracy_score(y_test,y_pred_lr_clf_countVec))\n# print(\"precision:\",metrics.precision_score(y_test,y_pred_lr_clf_countVec))\n# print(\"recall:\",metrics.recall_score(y_test,y_pred_lr_clf_countVec))\n# print(\"Confusion Matrix\")\n# sns.heatmap(metrics.confusion_matrix(y_test,y_pred_lr_clf_countVec),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:51:32.393102Z","iopub.execute_input":"2022-05-09T17:51:32.393688Z","iopub.status.idle":"2022-05-09T17:53:24.561813Z","shell.execute_reply.started":"2022-05-09T17:51:32.393647Z","shell.execute_reply":"2022-05-09T17:53:24.560822Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer=CountVectorizer()\n\ncountVec_train=count_vectorizer.fit_transform(x_train) \ncountVec_test=count_vectorizer.transform(x_test)\nprint(\"count_train : \", countVec_train.shape)\nprint(\"count_test : \", countVec_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:55:36.341724Z","iopub.execute_input":"2022-05-09T17:55:36.341976Z","iopub.status.idle":"2022-05-09T17:55:56.493631Z","shell.execute_reply.started":"2022-05-09T17:55:36.341950Z","shell.execute_reply":"2022-05-09T17:55:56.492891Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"en_stopwords=set(stopwords.words(\"english\"))\njoblib.dump(en_stopwords,\"stopwords.pkl\") \njoblib.dump(model,\"model.pkl\")\njoblib.dump(count_vectorizer,\"vectorizer.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T17:56:19.461845Z","iopub.execute_input":"2022-05-09T17:56:19.462110Z","iopub.status.idle":"2022-05-09T17:56:25.221756Z","shell.execute_reply.started":"2022-05-09T17:56:19.462081Z","shell.execute_reply":"2022-05-09T17:56:25.221062Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}